{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_all_fft(filepath):\n",
    "    mat = scipy.io.loadmat(filepath)\n",
    "    all_fft = mat['all_fft']\n",
    "    return all_fft\n",
    "\n",
    "def get_entropy(all_fft):\n",
    "    mat_dict = {'fft_band_mean':0,\n",
    "        'fft_band_array':1,\n",
    "        'fft_band_smooth_mean':2,\n",
    "        'fft_band_smooth_array':3, \n",
    "        'chan':4,\n",
    "        'file':5,\n",
    "        'task':6,\n",
    "        'time':7,\n",
    "        'raw':8, \n",
    "        'filter_data_array':9, \n",
    "        'psd_ratio_array':10}\n",
    "    \n",
    "    #different parameters\n",
    "    chan_list = []\n",
    "    subject_list = []\n",
    "    task_list = []\n",
    "    trial_index_list= []\n",
    "    task_type_list = []\n",
    "    task_type_idx_list = []\n",
    "    day_list = []\n",
    "      \n",
    "        \n",
    "    #observed variables: fft magnitue, power ratio\n",
    "    entropy_list = []     \n",
    "    df = pd.DataFrame(columns=['chan','subject','task','entropy'])   \n",
    "    trial_index = 0\n",
    "    trial_index_start = 0\n",
    "       \n",
    "    for i in range(all_fft.shape[1]):\n",
    "        if i == 0:\n",
    "            pre_trial = all_fft[0,i][mat_dict['task']][0]\n",
    "            pre_sub = all_fft[0,i][mat_dict['file']][0]\n",
    "\n",
    "        curr_trial = all_fft[0,i][mat_dict['task']][0]\n",
    "        curr_sub = all_fft[0,i][mat_dict['file']][0]\n",
    "\n",
    "        if pre_trial == curr_trial and pre_sub==curr_sub: \n",
    "            trial_index_start = trial_index\n",
    "            #same subject and same task, not change the trial_index label\n",
    "            pass\n",
    "        else:\n",
    "            trial_index = trial_index_start\n",
    "\n",
    "        pre_trial = curr_trial\n",
    "        pre_sub = curr_sub\n",
    "                \n",
    "        all_ts = all_fft[0,i][mat_dict['filter_data_array']][0]\n",
    "        step = 50       \n",
    "        print(i)\n",
    "        \n",
    "        \n",
    "        chan = [all_fft[0,i][mat_dict['chan']][0][0]]\n",
    "        day = [all_fft[0,i][mat_dict['time']][0]]\n",
    "        subject = [all_fft[0,i][mat_dict['file']][0]]\n",
    "        task = [all_fft[0,i][mat_dict['task']][0]]\n",
    "        task_type = [all_fft[0,i][mat_dict['task']][0][0:-1]]\n",
    "        task_type_index = [all_fft[0,i][mat_dict['task']][0]]\n",
    "        \n",
    "        for j in range(0,len(all_ts)-250,step):\n",
    "            ts = all_ts[j:j+249]\n",
    "            std_ts = np.std(ts)\n",
    "            sample_entropy = ent.sample_entropy(ts, 3, 0.25*std_ts)[2]\n",
    "            \n",
    "            entropy_list.append(sample_entropy)            \n",
    "            chan_list.extend(chan) #channel\n",
    "            subject_list.extend(subject)\n",
    "            task_list.extend(task)                     \n",
    "            trial_index_list.append(trial_index_start)\n",
    "            trial_index_start = trial_index_start + 1\n",
    "            \n",
    "            task_type_list.extend(task_type)\n",
    "            task_type_idx_list.extend(task_type_index)   \n",
    "            day_list.append(day)\n",
    "\n",
    "    df['chan'] = chan_list\n",
    "    df['day']=day_list\n",
    "    df['task'] = task_list\n",
    "    df['subject'] = subject_list\n",
    "    df['trial_index']=trial_index_list\n",
    "    df['trial_type'] = task_type_list\n",
    "    df['trial_type_idx'] = task_type_idx_list    \n",
    "    df['entropy']= entropy_list\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#export the pandas dataframe from the all_fft data\n",
    "def all_fft_to_df(all_fft):\n",
    "\n",
    "    #sub: chosen subject list\n",
    "    #val: fft magnitude, 0 v.s. power ratio, 1, default: fft magnitude\n",
    "    \n",
    "    #the column of the mat file\n",
    "    mat_dict = {'fft_band_mean':0,\n",
    "            'fft_band_array':1,\n",
    "            'fft_band_smooth_mean':2,\n",
    "            'fft_band_smooth_array':3, \n",
    "            'chan':4,\n",
    "            'file':5,\n",
    "            'task':6,\n",
    "            'time':7,\n",
    "            'raw':8, \n",
    "            'filter_data_array':9, \n",
    "            'psd_ratio_array':10}\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(columns=['chan','subject','task','freq','fft_band_smooth', 'psd_ratio'])\n",
    "\n",
    "    #different parameters\n",
    "    chan_list = []\n",
    "    subject_list = []\n",
    "    task_list = []\n",
    "    freq_list = []\n",
    "    trial_index_list= []\n",
    "    task_type_list = []\n",
    "    task_type_idx_list = []\n",
    "    day_list = []\n",
    "    \n",
    "    #observed variables: fft magnitue, power ratio\n",
    "    fft_list = []\n",
    "    raw_fft_list = []\n",
    "    psd_list = []\n",
    "    \n",
    "\n",
    "    trial_index_start = 0\n",
    "\n",
    "    for i in range(all_fft.shape[1]):\n",
    "        if i == 0:\n",
    "            pre_trial = all_fft[0,i][mat_dict['task']][0]\n",
    "            pre_sub = all_fft[0,i][mat_dict['file']][0]\n",
    "\n",
    "\n",
    "        curr_trial = all_fft[0,i][mat_dict['task']][0]\n",
    "        curr_sub = all_fft[0,i][mat_dict['file']][0]\n",
    "\n",
    "\n",
    "        if pre_trial == curr_trial and pre_sub==curr_sub:        \n",
    "            #same subject and same task, not change the trial_index label\n",
    "            pass\n",
    "        else:\n",
    "            trial_index_start = trial_index_start+len_fft\n",
    "\n",
    "        pre_trial = curr_trial\n",
    "        pre_sub = curr_sub\n",
    "        \n",
    "        raw_fft = all_fft[0,i][mat_dict['fft_band_array']][[0,1,2,4],1:] #first two row: alpha, beta1, beta2, theta\n",
    "        fft = all_fft[0,i][mat_dict['fft_band_smooth_array']][[0,1,2,4],1:] #first two row: alpha, beta1, beta2, theta\n",
    "        psd = all_fft[0,i][mat_dict['psd_ratio_array']][:,1:] #first two row: alpha, beta\n",
    "        len_fft = fft.shape[1]\n",
    "\n",
    "        chan = [all_fft[0,i][mat_dict['chan']][0][0]] * len_fft\n",
    "        day = [all_fft[0,i][mat_dict['time']][0]] * len_fft\n",
    "        subject = [all_fft[0,i][mat_dict['file']][0]] *len_fft\n",
    "        task = [all_fft[0,i][mat_dict['task']][0]] *len_fft\n",
    "        task_type = [all_fft[0,i][mat_dict['task']][0][0:-1]] *len_fft\n",
    "        task_type_index = [all_fft[0,i][mat_dict['task']][0]] *len_fft\n",
    "\n",
    "        trial_index = range(trial_index_start, trial_index_start+len_fft)\n",
    "\n",
    "        for j in range(4):               \n",
    "            freq = [j]*len_fft\n",
    "\n",
    "            chan_list.extend(chan) #channel\n",
    "            subject_list.extend(subject)\n",
    "            task_list.extend(task)      \n",
    "            freq_list.extend(freq)                  \n",
    "            trial_index_list.extend(trial_index)\n",
    "            task_type_list.extend(task_type)\n",
    "            task_type_idx_list.extend(task_type_index)\n",
    "            day_list.extend(day)\n",
    "            \n",
    "            fft_list.extend(fft[j,:]) \n",
    "            raw_fft_list.extend(raw_fft[j,:])\n",
    "            psd_list.extend(psd[j,:])\n",
    "\n",
    "\n",
    "    df['chan'] = chan_list\n",
    "    df['day'] = day_list\n",
    "    df['task'] = task_list\n",
    "    df['subject'] = subject_list\n",
    "    df['freq']=freq_list  \n",
    "    df['trial_index']=trial_index_list\n",
    "    df['trial_type'] = task_type_list\n",
    "    df['trial_type_idx'] = task_type_idx_list\n",
    "    \n",
    "    df['fft_band_smooth']= fft_list \n",
    "    df['fft_band_raw']=  raw_fft_list\n",
    "    df['psd_ratio'] = psd_list\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_feature_from_df(df, sub, task_list,select_chan, val=0):  \n",
    "    select_chan_freq = []\n",
    "    for i in select_chan:\n",
    "        for j in range(4):\n",
    "            select_chan_freq.append(str(i)+str(j))\n",
    "\n",
    "\n",
    "    if val == 0: #fft\n",
    "        df_select = df[df['subject'].isin(sub)][['chan', 'freq', 'fft_band_smooth', 'trial_index',\n",
    "                                                 'trial_type', 'trial_type_idx','day']]\n",
    "        df_select['chanfreq'] = df_select['chan'].apply(str)+ df_select['freq'].apply(str)\n",
    "        df_feature_pivot = df_select.pivot(index='trial_index', columns='chanfreq', values='fft_band_smooth')\n",
    "        df_class_pivot = df_select.pivot(index='trial_index', columns='chanfreq', values='trial_type')  \n",
    "        df_day_pivot = df_select.pivot(index='trial_index', columns='chanfreq', values='day')  \n",
    "        \n",
    "        X = df_feature_pivot[select_chan_freq]\n",
    "        y, uniques = pd.factorize(df_class_pivot['10'],sort = True)\n",
    "        day, uniques_day = pd.factorize(df_day_pivot['10'],sort = True)\n",
    "        \n",
    "    elif val == 1: #power ratio\n",
    "        df_select = df[df['subject'].isin(sub)][['chan', 'freq', 'psd_ratio', 'trial_index',\n",
    "                                                 'trial_type', 'trial_type_idx','day']]\n",
    "        df_select['chanfreq'] = df_select['chan'].apply(str)+ df_select['freq'].apply(str)\n",
    "        df_feature_pivot = df_select.pivot(index='trial_index', columns='chanfreq', values='psd_ratio')\n",
    "        df_class_pivot = df_select.pivot(index='trial_index', columns='chanfreq', values='trial_type')\n",
    "        df_day_pivot = df_select.pivot(index='trial_index', columns='chanfreq', values='day')\n",
    "                \n",
    "        \n",
    "        X = df_feature_pivot[select_chan_freq]\n",
    "        y, uniques = pd.factorize(df_class_pivot['10'],sort = True)\n",
    "        day, uniques_day = pd.factorize(df_day_pivot['10'],sort = True)\n",
    "\n",
    "              \n",
    "    elif val == 2: #entropy\n",
    "        df_select = df[df['subject'].isin(sub)][['chan', 'entropy', 'trial_index','trial_type', 'trial_type_idx']]\n",
    "        df_select['chan'] = df_select['chan'].apply(str)\n",
    "        df_feature_pivot = df_select.pivot(index='trial_index', columns='chan', values='entropy')\n",
    "        df_class_pivot = df_select.pivot(index='trial_index', columns='chan', values='trial_type')\n",
    "        \n",
    "        X = df_feature_pivot[['1', '2', '3','4']]\n",
    "        y, uniques = pd.factorize(df_class_pivot['1'],sort = True)\n",
    "        \n",
    "                \n",
    "    elif val == 3: #raw fft\n",
    "        df_select = df[df['subject'].isin(sub)][['chan', 'freq', 'fft_band_raw', 'trial_index','trial_type', 'trial_type_idx']]\n",
    "        df_select['chanfreq'] = df_select['chan'].apply(str)+ df_select['freq'].apply(str)\n",
    "        df_feature_pivot = df_select.pivot(index='trial_index', columns='chanfreq', values='fft_band_raw')\n",
    "        df_class_pivot = df_select.pivot(index='trial_index', columns='chanfreq', values='trial_type')  \n",
    "        \n",
    "        X = df_feature_pivot[select_chan_freq]\n",
    "        y, uniques = pd.factorize(df_class_pivot['10'],sort = True)\n",
    "                \n",
    "        \n",
    "    else:\n",
    "        print('please choose power ratio or fft magnitude or entropy for analysis')\n",
    "        \n",
    "    \n",
    "    x_select = X.iloc[[i for i, v in enumerate(uniques.take(y)) if v in task_list],:]\n",
    "    y_select = y[[i for i, v in enumerate(uniques.take(y)) if v in task_list]]\n",
    "    day_select = day[[i for i, v in enumerate(uniques.take(y)) if v in task_list]]\n",
    "        \n",
    "\n",
    "    return x_select, y_select, uniques, day\n",
    "\n",
    "    #X, the feature of the channels and alpha, beta amplitude\n",
    "    #y, the target of the task; 'L1', 'L2', 'L3', 'L4', 'eyeclose', 'eyeopen'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_raw_data(data):\n",
    "    #normalize data\n",
    "    #electrode-wise exponential moving standardization  \n",
    "    \n",
    "    #input: data, with shape (1, time)\n",
    "    #first 1000 sample as average\n",
    "    m = np.mean(data[:,0:1000], axis = 1) #m0 is the first 1000 datapoints mean values\n",
    "    v = np.var(data[:,0:1000],axis = 1) #v0 is the first 1000 datapoints variance values\n",
    "    normalize_data = np.zeros((data.shape[0], data.shape[1]))\n",
    "\n",
    "    for i in range(1000,data.shape[1]):\n",
    "        m = 0.001*data[:,i] + 0.999*m\n",
    "        v = 0.001*((data[:,i]-m)**2) + 0.999*v\n",
    "        normalize_data[:,i] = (data[:,i]-m)/np.sqrt(v)\n",
    "        \n",
    "    return normalize_data\n",
    "\n",
    "def get_raw_data(all_fft, sub, select_task_list, day):\n",
    "    mat_dict = {'fft_band_mean':0,\n",
    "        'fft_band_array':1,\n",
    "        'fft_band_smooth_mean':2,\n",
    "        'fft_band_smooth_array':3, \n",
    "        'chan':4,\n",
    "        'file':5,\n",
    "        'task':6,\n",
    "        'time':7,\n",
    "        'raw':8, \n",
    "        'filter_data_array':9, \n",
    "        'psd_ratio_array':10}\n",
    "\n",
    "    chan_list = []\n",
    "    data_list = []\n",
    "    task_list = []\n",
    "    task_type_list = []\n",
    "\n",
    "    for i in range(all_fft.shape[1]):\n",
    "        curr_sub = all_fft[0,i][mat_dict['file']][0]\n",
    "        task_type = all_fft[0,i][mat_dict['task']][0][0:-1]\n",
    "        chan = all_fft[0,i][mat_dict['chan']][0][0]\n",
    "        task = all_fft[0,i][mat_dict['task']][0]\n",
    "\n",
    "        if curr_sub in sub and task_type in select_task_list and chan in [2,4]:\n",
    "            #update calculation in every 50 sample (with 250hz, that is every 1/5 second)\n",
    "            all_ts = all_fft[0,i][mat_dict['filter_data_array']]\n",
    "            chan_list.append(chan)\n",
    "            data_list.append(all_ts)\n",
    "            task_list.append(task)\n",
    "            task_type_list.append(task_type)\n",
    "\n",
    "    df = pd.DataFrame({'chan':chan_list,\n",
    "                      'data': data_list,\n",
    "                      'task': task_list,\n",
    "                      'task_type':task_type_list})\n",
    "\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    day_list = []\n",
    "    step = 50\n",
    "\n",
    "    for tsk in np.unique(task_list):\n",
    "        tmp1 = df[(df['chan'] == 2) & (df['task'] == tsk)]['data'].values[0]\n",
    "        tmp2 = df[(df['chan'] == 4) & (df['task'] == tsk)]['data'].values[0]\n",
    "        tmp3 = np.concatenate([tmp1, tmp2], axis = 0)\n",
    "        tmp4 = smooth_raw_data(tmp3) #normalized filter data\n",
    "        for i in range(1000, tmp4.shape[1]-251,step):\n",
    "            ts = tmp4[:, i:i+250]\n",
    "            X.append(ts)\n",
    "            y.append(tsk[:-1])\n",
    "            day_list.append(day)\n",
    "\n",
    "\n",
    "    X = np.stack([x for x in X], axis = 0) \n",
    "    \n",
    "    return X,y,day_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_from_df(df, sub, task_list,select_chan, val=0):  \n",
    "    select_chan_freq = []\n",
    "    for i in select_chan:\n",
    "        for j in range(4):\n",
    "            select_chan_freq.append(str(i)+str(j))\n",
    "\n",
    "\n",
    "    if val == 0: #fft\n",
    "        df_select = df[df['subject'].isin(sub)][['chan', 'freq', 'fft_band_smooth', 'trial_index',\n",
    "                                                 'trial_type', 'trial_type_idx','day']]\n",
    "        df_select['chanfreq'] = df_select['chan'].apply(str)+ df_select['freq'].apply(str)\n",
    "        df_feature_pivot = df_select.pivot(index='trial_index', columns='chanfreq', values='fft_band_smooth')\n",
    "        df_class_pivot = df_select.pivot(index='trial_index', columns='chanfreq', values='trial_type')  \n",
    "        df_day_pivot = df_select.pivot(index='trial_index', columns='chanfreq', values='day')  \n",
    "        \n",
    "        X = df_feature_pivot[select_chan_freq]\n",
    "        y, uniques = pd.factorize(df_class_pivot['10'],sort = True)\n",
    "        day,uniques_day = pd.factorize(df_day_pivot['10'],sort = True)\n",
    "        \n",
    "    elif val == 1: #power ratio\n",
    "        df_select = df[df['subject'].isin(sub)][['chan', 'freq', 'psd_ratio', 'trial_index',\n",
    "                                                 'trial_type', 'trial_type_idx','day']]\n",
    "        df_select['chanfreq'] = df_select['chan'].apply(str)+ df_select['freq'].apply(str)\n",
    "        df_feature_pivot = df_select.pivot(index='trial_index', columns='chanfreq', values='psd_ratio')\n",
    "        df_class_pivot = df_select.pivot(index='trial_index', columns='chanfreq', values='trial_type')\n",
    "        df_day_pivot = df_select.pivot(index='trial_index', columns='chanfreq', values='day')\n",
    "                \n",
    "        \n",
    "        X = df_feature_pivot[select_chan_freq]\n",
    "        y, uniques = pd.factorize(df_class_pivot['10'],sort = True)\n",
    "        day,uniques_day = pd.factorize(df_day_pivot['10'],sort = True)\n",
    "\n",
    "              \n",
    "    elif val == 2: #entropy\n",
    "        df_select = df[df['subject'].isin(sub)][['chan', 'entropy', 'trial_index','trial_type', 'trial_type_idx']]\n",
    "        df_select['chan'] = df_select['chan'].apply(str)\n",
    "        df_feature_pivot = df_select.pivot(index='trial_index', columns='chan', values='entropy')\n",
    "        df_class_pivot = df_select.pivot(index='trial_index', columns='chan', values='trial_type')\n",
    "        \n",
    "        X = df_feature_pivot[['1', '2', '3','4']]\n",
    "        y, uniques = pd.factorize(df_class_pivot['1'],sort = True)\n",
    "        \n",
    "                \n",
    "    elif val == 3: #raw fft\n",
    "        df_select = df[df['subject'].isin(sub)][['chan', 'freq', 'fft_band_raw', 'trial_index','trial_type', 'trial_type_idx']]\n",
    "        df_select['chanfreq'] = df_select['chan'].apply(str)+ df_select['freq'].apply(str)\n",
    "        df_feature_pivot = df_select.pivot(index='trial_index', columns='chanfreq', values='fft_band_raw')\n",
    "        df_class_pivot = df_select.pivot(index='trial_index', columns='chanfreq', values='trial_type')  \n",
    "        \n",
    "        X = df_feature_pivot[select_chan_freq]\n",
    "        y, uniques = pd.factorize(df_class_pivot['10'],sort = True)\n",
    "                \n",
    "        \n",
    "    else:\n",
    "        print('please choose power ratio or fft magnitude or entropy for analysis')\n",
    "        \n",
    "    \n",
    "    x_select = X.iloc[[i for i, v in enumerate(uniques.take(y)) if v in task_list],:]\n",
    "    y_select = y[[i for i, v in enumerate(uniques.take(y)) if v in task_list]]\n",
    "    day_select = day[[i for i, v in enumerate(uniques.take(y)) if v in task_list]]\n",
    "        \n",
    "\n",
    "    return x_select, y_select, uniques, day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare train and test data (3 level, for 8 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all 5 levels \n",
    "all_fft = get_all_fft('C:\\\\Users\\\\OWNER\\\\Downloads\\\\BCI_MATH_ALL_FFT\\\\level3_5freq_step250_all_fft.mat')\n",
    "df = all_fft_to_df(all_fft)\n",
    "\n",
    "X_train, y_train, uniques, day = get_feature_from_df(df, ['fzb'],['l1','l3','l5'], [2,4],0)\n",
    "\n",
    "X_train['y'] = y_train()\n",
    "X_train['day']=day\n",
    "X_train.to_csv('D:/BCI_Classification/bci_fft_5freq_data_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(X_train['day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_fft9_1 = get_all_fft('C:\\\\Users\\\\OWNER\\\\Downloads\\\\BCI_MATH_ALL_FFT\\\\20200227_level3_sameday_5freq_step250_all_fft_0.mat')\n",
    "df_9_1 = all_fft_to_df(all_fft9_1)\n",
    "\n",
    "all_fft9_2 = get_all_fft('C:\\\\Users\\\\OWNER\\\\Downloads\\\\BCI_MATH_ALL_FFT\\\\20200227_level3_sameday_5freq_step250_all_fft_2.mat')\n",
    "df_9_2 = all_fft_to_df(all_fft9_2)\n",
    "\n",
    "X_train, y_train, uniques_9, day = get_feature_from_df(df_9_1, ['fzb'],['l1','l3','l5'], [2,4],0)\n",
    "X_test, y_test, uniques_10, day = get_feature_from_df(df_9_2, ['fzb'],['l1','l3','l5'], [2,4], 0)\n",
    "\n",
    "\n",
    "X_train['y'] = y_train\n",
    "X_test['y'] = y_test\n",
    "\n",
    "# # # # X['day'] = day\n",
    "X_train.to_csv('D:/BCI_Classification/20200227_level3_sameday_5freq_step250_all_fft_0.csv')\n",
    "X_test.to_csv('D:/BCI_Classification/20200227_level3_sameday_5freq_step250_all_fft_2.csv')\n",
    "\n",
    "# # # change to 0 base\n",
    "# y_train = X_train['y']\n",
    "# y_test = X_test['y']\n",
    "\n",
    "# for j in X_train.columns[1:-1]:\n",
    "#     fig, (ax1, ax2) = plt.subplots(1,2,figsize = (8,3))\n",
    "\n",
    "#     for i in range(5):\n",
    "#         ax1.hist(X_train.loc[y_train==i,j], label = str(i), bins = 30)\n",
    "#         ax2.hist(X_test.loc[y_train==i,j], label = str(i), bins = 30)\n",
    "        \n",
    "#     ax1.set_title(j+'train')\n",
    "#     ax2.set_title(j+'test')\n",
    "    \n",
    "\n",
    "#     ax1.set_xlim(0.2,1)  \n",
    "#     ax2.set_xlim(0.2,1)  \n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data for raw data + eegnet analysis (5 class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_list = []\n",
    "y_list = []\n",
    "day_list = []\n",
    "for i in range(4,9):\n",
    "    line = 'get_raw_data(all_fft{idx},[\"fzb\"],[\"l1\",\"l2\",\"l3\",\"l4\",\"l5\"], {idx})'.format(idx = i)\n",
    "    try:\n",
    "        [X_tmp,y_tmp,d_tmp] = eval(line)\n",
    "        X_list.append(X_tmp)\n",
    "        y_list.extend(y_tmp)\n",
    "        day_list.extend(d_tmp)\n",
    "    except (NameError, SyntaxError):\n",
    "        print(line)\n",
    "        \n",
    "X = np.concatenate([x for x in X_list], axis = 0)\n",
    "[y,target_name] = pd.factorize(y_list)\n",
    "\n",
    "np.save('raw_data_X.npy',X)\n",
    "np.save('raw_data_y.npy',y)\n",
    "np.save('raw_data_target_name.npy',target_name)\n",
    "np.save('raw_data_day.npy',day_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fzb, all five days, 3 levels, write to csv, fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_0, y_train_0, uniques_0 = get_feature_from_df(df_0, ['fzb'],['l1','l2','l3'], 0)\n",
    "X_train_1, y_train_1, uniques_1 = get_feature_from_df(df_1, ['fzb'],['l1','l2','l3'], 0)\n",
    "#X_train_2, y_train_2, uniques_2 = get_feature_from_df(df_2, ['fzb'],['l1','l2','l3'], 0)\n",
    "X_train_3, y_train_3, uniques_3 = get_feature_from_df(df_3, ['fzb'],['l1','l2','l4'], 0)\n",
    "X_train_4, y_train_4, uniques_4 = get_feature_from_df(df_4, ['fzb'],['l1','l2','l4'], 0)\n",
    "X_train_5, y_train_5, uniques_5 = get_feature_from_df(df_5, ['fzb'],['l1','l2','l4'], 0)\n",
    "X_train_6, y_train_6, uniques_6 = get_feature_from_df(df_6, ['fzb'],['l1','l2','l4'], 0)\n",
    "X_train_7, y_train_7, uniques_7 = get_feature_from_df(df_7, ['fzb'],['l1','l2','l4'], 0)\n",
    "X_train_8, y_train_8, uniques_8 = get_feature_from_df(df_8, ['fzb'],['l1','l2','l4'], 0)\n",
    "\n",
    "#this dataset has level 4 same as the other dataset levl 3\n",
    "y_train_3[y_train_3 ==5 ] = 4\n",
    "y_train_4[y_train_4 ==5 ] = 4\n",
    "y_train_5[y_train_5 ==5 ] = 4\n",
    "y_train_6[y_train_6 ==5 ] = 4\n",
    "y_train_7[y_train_7 ==5 ] = 4\n",
    "y_train_8[y_train_8 ==5 ] = 4\n",
    "\n",
    "\n",
    "X = pd.concat([X_train_0, X_train_1, X_train_3, X_train_4,\n",
    "               X_train_5, X_train_6, X_train_7, X_train_8], axis = 0)\n",
    "\n",
    "y = np.concatenate([y_train_0, y_train_1, y_train_3, y_train_4, \n",
    "                    y_train_5, y_train_6, y_train_7, y_train_8], axis = 0)\n",
    "\n",
    "day = np.concatenate([[1]* y_train_0.shape[0], [2]* y_train_1.shape[0], \n",
    "                     [3]* y_train_3.shape[0], [4]* y_train_4.shape[0], \n",
    "                     [5]* y_train_5.shape[0], [6]* y_train_6.shape[0], \n",
    "                     [7]*y_train_7.shape[0], [8]*y_train_8.shape[0]], axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "X = X[['20','21','40','41']]\n",
    "\n",
    "X['y'] = y\n",
    "\n",
    "X['day'] = day\n",
    "X.to_csv('D:/BCI_Classification/bci_fft_3_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# same day different time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, uniques_9 = get_feature_from_df(df_9_1, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "X_test, y_test, uniques_10 = get_feature_from_df(df_9_2, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "\n",
    "\n",
    "\n",
    "# day = np.concatenate([[1]* y_train_0.shape[0], [2]* y_train_1.shape[0], \n",
    "#                      [3]* y_train_3.shape[0], [4]* y_train_4.shape[0], \n",
    "#                      [5]* y_train_5.shape[0], [6]* y_train_6.shape[0], \n",
    "#                      [7]*y_train_7.shape[0], [8]*y_train_8.shape[0]], axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train = X_train[['20','21','40','41']]\n",
    "X_test = X_test[['20','21','40','41']]\n",
    "\n",
    "X_train['y'] = y_train\n",
    "X_test['y'] = y_test\n",
    "\n",
    "# X['day'] = day\n",
    "X_train.to_csv('D:/BCI_Classification/bci_fft_5_sameday_data_train.csv')\n",
    "X_test.to_csv('D:/BCI_Classification/bci_fft_5_sameday_data_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, uniques_9 = get_feature_from_df(df_en_9_1, ['fzb'],['l1','l2','l3','l4','l5'], 2)\n",
    "X_test, y_test, uniques_10 = get_feature_from_df(df_en_9_2, ['fzb'],['l1','l2','l3','l4','l5'], 2)\n",
    "\n",
    "\n",
    "\n",
    "# day = np.concatenate([[1]* y_train_0.shape[0], [2]* y_train_1.shape[0], \n",
    "#                      [3]* y_train_3.shape[0], [4]* y_train_4.shape[0], \n",
    "#                      [5]* y_train_5.shape[0], [6]* y_train_6.shape[0], \n",
    "#                      [7]*y_train_7.shape[0], [8]*y_train_8.shape[0]], axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train = X_train[['2','4']]\n",
    "X_test = X_test[['2','4']]\n",
    "\n",
    "X_train['y'] = y_train\n",
    "X_test['y'] = y_test\n",
    "\n",
    "# X['day'] = day\n",
    "X_train.to_csv('D:/BCI_Classification/bci_entropy_5_sameday_data_train.csv')\n",
    "X_test.to_csv('D:/BCI_Classification/bci_entropy_5_sameday_data_test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_0, y_train_0, uniques_0 = get_feature_from_df(df_en_0, ['fzb'],['l1','l2','l3'], 2)\n",
    "X_train_1, y_train_1, uniques_1 = get_feature_from_df(df_en_1, ['fzb'],['l1','l2','l3'], 2)\n",
    "#X_train_2, y_train_2, uniques_2 = get_feature_from_df(df_2, ['fzb'],['l1','l2','l3'], 2)\n",
    "X_train_3, y_train_3, uniques_3 = get_feature_from_df(df_en_3, ['fzb'],['l1','l2','l4'], 2)\n",
    "X_train_4, y_train_4, uniques_4 = get_feature_from_df(df_en_4, ['fzb'],['l1','l2','l4'], 2)\n",
    "X_train_5, y_train_5, uniques_5 = get_feature_from_df(df_en_5, ['fzb'],['l1','l2','l4'], 2)\n",
    "X_train_6, y_train_6, uniques_6 = get_feature_from_df(df_en_6, ['fzb'],['l1','l2','l4'], 2)\n",
    "X_train_7, y_train_7, uniques_7 = get_feature_from_df(df_en_7, ['fzb'],['l1','l2','l4'], 2)\n",
    "X_train_8, y_train_8, uniques_8 = get_feature_from_df(df_en_8, ['fzb'],['l1','l2','l4'], 2)\n",
    "\n",
    "#this dataset has level 4 same as the other dataset levl 3\n",
    "y_train_3[y_train_3 ==5 ] = 4\n",
    "y_train_4[y_train_4 ==5 ] = 4\n",
    "y_train_5[y_train_5 ==5 ] = 4\n",
    "y_train_6[y_train_6 ==5 ] = 4\n",
    "y_train_7[y_train_7 ==5 ] = 4\n",
    "y_train_8[y_train_8 ==5 ] = 4\n",
    "\n",
    "\n",
    "X = pd.concat([X_train_0, X_train_1, X_train_3, X_train_4,\n",
    "               X_train_5, X_train_6, X_train_7, X_train_8], axis = 0)\n",
    "\n",
    "y = np.concatenate([y_train_0, y_train_1, y_train_3, y_train_4, \n",
    "                    y_train_5, y_train_6, y_train_7, y_train_8], axis = 0)\n",
    "\n",
    "day = np.concatenate([[1]* y_train_0.shape[0], [2]* y_train_1.shape[0], \n",
    "                     [3]* y_train_3.shape[0], [4]* y_train_4.shape[0], \n",
    "                     [5]* y_train_5.shape[0], [6]* y_train_6.shape[0], \n",
    "                     [7]*y_train_7.shape[0], [8]*y_train_8.shape[0]], axis = 0)\n",
    "\n",
    "X = X[['2','4']]\n",
    "\n",
    "X['y'] = y\n",
    "\n",
    "X['day'] = day\n",
    "X.to_csv('D:/BCI_Classification/bci_entropy_3_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fzb, all four days, 5 levels, write to csv, fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_4, y_train_4, uniques_4 = get_feature_from_df(df_4, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "X_train_5, y_train_5, uniques_5 = get_feature_from_df(df_5, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "X_train_6, y_train_6, uniques_6 = get_feature_from_df(df_6, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "X_train_7, y_train_7, uniques_7 = get_feature_from_df(df_7, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "X_train_8, y_train_8, uniques_8 = get_feature_from_df(df_8, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "\n",
    "#this dataset has level 4 same as the other dataset levl 3\n",
    "\n",
    "\n",
    "X = pd.concat([X_train_4, X_train_5, X_train_6, X_train_7, X_train_8], axis = 0)\n",
    "\n",
    "y = np.concatenate([y_train_4, y_train_5, y_train_6, y_train_7, y_train_8], axis = 0)\n",
    "\n",
    "day = np.concatenate([[1]* y_train_4.shape[0], [2]* y_train_5.shape[0], \n",
    "                     [3]* y_train_6.shape[0], [4]* y_train_7.shape[0], \n",
    "                      [5]*y_train_8.shape[0]],  axis = 0)\n",
    "\n",
    "X  = X[['20','21','40','41']]\n",
    "\n",
    "X['y'] = y\n",
    "X['day'] = day\n",
    "X.to_csv('D:/BCI_Classification/bci_fft_5_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_4, y_train_4, uniques_4 = get_feature_from_df(df_en_4, ['fzb'],['l1','l2','l3','l4','l5'], 2)\n",
    "X_train_5, y_train_5, uniques_5 = get_feature_from_df(df_en_5, ['fzb'],['l1','l2','l3','l4','l5'], 2)\n",
    "X_train_6, y_train_6, uniques_6 = get_feature_from_df(df_en_6, ['fzb'],['l1','l2','l3','l4','l5'], 2)\n",
    "X_train_7, y_train_7, uniques_7 = get_feature_from_df(df_en_7, ['fzb'],['l1','l2','l3','l4','l5'], 2)\n",
    "X_train_8, y_train_8, uniques_8 = get_feature_from_df(df_en_8, ['fzb'],['l1','l2','l3','l4','l5'], 2)\n",
    "\n",
    "#this dataset has level 4 same as the other dataset levl 3\n",
    "\n",
    "\n",
    "X = pd.concat([X_train_4, X_train_5, X_train_6, X_train_7, X_train_8], axis = 0)\n",
    "\n",
    "y = np.concatenate([y_train_4, y_train_5, y_train_6, y_train_7, y_train_8], axis = 0)\n",
    "\n",
    "day = np.concatenate([[1]* y_train_4.shape[0], [2]* y_train_5.shape[0], \n",
    "                     [3]* y_train_6.shape[0], [4]* y_train_7.shape[0], \n",
    "                      [5]*y_train_8.shape[0]],  axis = 0)\n",
    "\n",
    "X  = X[['2','4']]\n",
    "\n",
    "X['y'] = y\n",
    "X['day'] = day\n",
    "X.to_csv('D:/BCI_Classification/bci_entropy_5_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_7, y_train_7, uniques_7 = get_feature_from_df(df_7, ['fzb'],['l1','l2','l3', 'l4', 'l5'], 0)\n",
    "uniques_tak = uniques_7\n",
    "X = X_train_7[['20','21','40','41']]\n",
    "y = y_train_7\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "clf = LogisticRegressionCV(cv=5, random_state=0,max_iter = 10000, multi_class = 'auto').fit(X, y)\n",
    "\n",
    "print(clf.score(X, y))\n",
    "#print('coefficient: '+str(clf.coef_))\n",
    "#np.savetxt(sub+'day0_coef.csv', clf.coef_)\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "X_transform = lda.fit_transform(X,y)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "colors = ['red', 'green', 'blue','yellow','purple']\n",
    "target_names = uniques_7[np.unique(y)]\n",
    "\n",
    "for color, i, target_name in zip(colors, np.unique(y), target_names):\n",
    "    plt.scatter(X_transform[y == i, 0], X_transform[y == i, 1], color=color, alpha=.8,# lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('lda')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "colors = ['red', 'green', 'blue','yellow','purple']\n",
    "target_names = uniques_7[np.unique(y)]\n",
    "\n",
    "for color, i, target_name in zip(colors, np.unique(y), target_names):\n",
    "    plt.hist(X_transform[y == i, 0], color=color, alpha=.8,# lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('LDA')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_6, y_train_6, uniques_6 = get_feature_from_df(df_7, ['fzb'],['l1','l2','l4'], 0)\n",
    "uniques_tak = uniques_6\n",
    "X = X_train_6[['20','21','40','41']]\n",
    "y = y_train_6\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "clf = LogisticRegressionCV(cv=5, random_state=0,max_iter = 10000, multi_class = 'auto').fit(X, y)\n",
    "\n",
    "print(clf.score(X, y))\n",
    "#print('coefficient: '+str(clf.coef_))\n",
    "#np.savetxt(sub+'day0_coef.csv', clf.coef_)\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "X_transform = lda.fit_transform(X,y)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "colors = ['red', 'green', 'blue','yellow']\n",
    "target_names = uniques_tak[np.unique(y)]\n",
    "\n",
    "for color, i, target_name in zip(colors, np.unique(y), target_names):\n",
    "    plt.scatter(X_transform[y == i, 0], X_transform[y == i, 1], color=color, alpha=.8,# lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('lda')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "colors = ['red', 'green', 'blue','yellow','purple']\n",
    "target_names = uniques_tak[np.unique(y)]\n",
    "\n",
    "for color, i, target_name in zip(colors, np.unique(y), target_names):\n",
    "    plt.hist(X_transform[y == i, 0], color=color, alpha=.8,# lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('LDA')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_7, y_train_7, uniques_7 = get_feature_from_df(df_7, ['fzb'],['l1','l2', 'l4'], 0)\n",
    "uniques_tak = uniques_7\n",
    "X = X_train_7[['20','21','40','41']]\n",
    "y = y_train_7\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "clf = LogisticRegressionCV(cv=5, random_state=0,max_iter = 10000, multi_class = 'auto').fit(X, y)\n",
    "\n",
    "print(clf.score(X, y))\n",
    "#print('coefficient: '+str(clf.coef_))\n",
    "#np.savetxt(sub+'day0_coef.csv', clf.coef_)\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "X_transform = lda.fit_transform(X,y)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "colors = ['red', 'green', 'blue','yellow','purple']\n",
    "target_names = uniques_7[np.unique(y)]\n",
    "\n",
    "for color, i, target_name in zip(colors, np.unique(y), target_names):\n",
    "    plt.scatter(X_transform[y == i, 0], X_transform[y == i, 1], color=color, alpha=.8,# lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('lda')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "colors = ['red', 'green', 'blue','yellow','purple']\n",
    "target_names = uniques_7[np.unique(y)]\n",
    "\n",
    "for color, i, target_name in zip(colors, np.unique(y), target_names):\n",
    "    plt.hist(X_transform[y == i, 0], color=color, alpha=.8,# lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('LDA')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## svm, grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "X_train_0, y_train_0, uniques_0 = get_feature_from_df(df_0, ['fzb'],['l1','l2','l3'], 0)\n",
    "X_train_1, y_train_1, uniques_1 = get_feature_from_df(df_1, ['fzb'],['l1','l2','l3'], 0)\n",
    "#X_train_2, y_train_2, uniques_2 = get_feature_from_df(df_2, ['fzb'],['l1','l2','l3'], 0)\n",
    "X_train_3, y_train_3, uniques_3 = get_feature_from_df(df_3, ['fzb'],['l1','l2','l4'], 0)\n",
    "X_train_4, y_train_4, uniques_4 = get_feature_from_df(df_4, ['fzb'],['l1','l2','l4'], 0)\n",
    "X_train_5, y_train_5, uniques_5 = get_feature_from_df(df_5, ['fzb'],['l1','l2','l4'], 0)\n",
    "X_train_6, y_train_6, uniques_6 = get_feature_from_df(df_6, ['fzb'],['l1','l2','l4'], 0)\n",
    "X_train_7, y_train_7, uniques_7 = get_feature_from_df(df_7, ['fzb'],['l1','l2','l4'], 0)\n",
    "\n",
    "#this dataset has level 4 same as the other dataset levl 3\n",
    "y_train_3[y_train_3 ==5 ] = 4\n",
    "y_train_4[y_train_4 ==5 ] = 4\n",
    "y_train_5[y_train_5 ==5 ] = 4\n",
    "y_train_6[y_train_6 ==5 ] = 4\n",
    "y_train_7[y_train_7 ==5 ] = 4\n",
    "\n",
    "X = pd.concat([X_train_0, X_train_1, X_train_3, X_train_4, X_train_5, X_train_6, X_train_7], axis = 0)\n",
    "y = np.concatenate([y_train_0, y_train_1, y_train_3,  y_train_4,  y_train_5, y_train_6, y_train_7], axis = 0)\n",
    "\n",
    "\n",
    "X  = X[['20','21','40','41']]\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        SVC(), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    #Classifier Confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    print('Confusion Matrix : \\n' + str(confusion_matrix(y_test,y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "X_train_0, y_train_0, uniques_0 = get_feature_from_df(df_0, ['fzb'],['l1','l2','l3'], 0)\n",
    "X_train_1, y_train_1, uniques_1 = get_feature_from_df(df_1, ['fzb'],['l1','l2','l3'], 0)\n",
    "#X_train_2, y_train_2, uniques_2 = get_feature_from_df(df_2, ['fzb'],['l1','l2','l3'], 0)\n",
    "X_train_3, y_train_3, uniques_3 = get_feature_from_df(df_3, ['fzb'],['l1','l2','l4'], 0)\n",
    "X_train_4, y_train_4, uniques_4 = get_feature_from_df(df_4, ['fzb'],['l1','l2','l4'], 0)\n",
    "X_train_5, y_train_5, uniques_5 = get_feature_from_df(df_5, ['fzb'],['l1','l2','l4'], 0)\n",
    "X_train_6, y_train_6, uniques_6 = get_feature_from_df(df_6, ['fzb'],['l1','l2','l4'], 0)\n",
    "X_train_7, y_train_7, uniques_7 = get_feature_from_df(df_7, ['fzb'],['l1','l2','l4'], 0)\n",
    "\n",
    "#this dataset has level 4 same as the other dataset levl 3\n",
    "y_train_3[y_train_3 ==5 ] = 4\n",
    "y_train_4[y_train_4 ==5 ] = 4\n",
    "y_train_5[y_train_5 ==5 ] = 4\n",
    "y_train_6[y_train_6 ==5 ] = 4\n",
    "y_train_7[y_train_7 ==5 ] = 4\n",
    "\n",
    "X = pd.concat([X_train_0, X_train_1, X_train_3, X_train_4, X_train_5, X_train_6, X_train_7], axis = 0)\n",
    "y = np.concatenate([y_train_0, y_train_1, y_train_3,  y_train_4,  y_train_5, y_train_6, y_train_7], axis = 0)\n",
    "\n",
    "X  = X[['20','21','40','41']]\n",
    "\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "onevsrest = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train, y_train).score(X_test, y_test)\n",
    "onevsone =  OneVsOneClassifier(LinearSVC(random_state=0)).fit(X_train, y_train).score(X_test, y_test)\n",
    "\n",
    "\n",
    "print(onevsrest)\n",
    "print(onevsone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_4, y_train_4, uniques_4 = get_feature_from_df(df_4, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "X_train_5, y_train_5, uniques_5 = get_feature_from_df(df_5, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "X_train_6, y_train_6, uniques_6 = get_feature_from_df(df_6, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "X_train_7, y_train_7, uniques_7 = get_feature_from_df(df_7, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "\n",
    "\n",
    "#this dataset has level 4 same as the other dataset levl 3\n",
    "\n",
    "\n",
    "X = pd.concat([ X_train_4, X_train_5,X_train_6,X_train_7], axis = 0)\n",
    "y = np.concatenate([y_train_4,  y_train_5, y_train_6, y_train_7], axis = 0)\n",
    "\n",
    "X  = X[['20','21','40','41']]\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "clf = LogisticRegressionCV(cv=5, random_state=0,max_iter = 10000, multi_class = 'auto').fit(X, y)\n",
    "\n",
    "print(clf.score(X, y))\n",
    "#print('coefficient: '+str(clf.coef_))\n",
    "#np.savetxt(sub+'day0_coef.csv', clf.coef_)\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "X_transform = lda.fit_transform(X,y)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "colors = ['red', 'green', 'blue','yellow','purple']\n",
    "target_names = uniques_5[np.unique(y)]\n",
    "\n",
    "for color, i, target_name in zip(colors, np.unique(y), target_names):\n",
    "    plt.scatter(X_transform[y == i, 0], X_transform[y == i, 1], color=color, alpha=.8,# lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('lda')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "colors = ['red', 'green', 'blue','yellow','purple']\n",
    "target_names = uniques_5[np.unique(y)]\n",
    "\n",
    "for color, i, target_name in zip(colors, np.unique(y), target_names):\n",
    "    plt.hist(X_transform[y == i, 0], color=color, alpha=.8,# lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('lda')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train_4, y_train_4, uniques_4 = get_feature_from_df(df_4, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "X_train_5, y_train_5, uniques_5 = get_feature_from_df(df_5, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "X_train_6, y_train_6, uniques_6 = get_feature_from_df(df_6, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "X_train_7, y_train_7, uniques_7 = get_feature_from_df(df_7, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "\n",
    "\n",
    "#this dataset has level 4 same as the other dataset levl 3\n",
    "\n",
    "\n",
    "X = pd.concat([ X_train_4, X_train_5,X_train_6,X_train_7], axis = 0)\n",
    "y = np.concatenate([y_train_4, y_train_5, y_train_6, y_train_7], axis = 0)\n",
    "\n",
    "X  = X[['20','21','40','41']]\n",
    "\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        SVC(), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_4, y_train_4, uniques_4 = get_feature_from_df(df_4, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "X_train_5, y_train_5, uniques_5 = get_feature_from_df(df_5, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "X_train_6, y_train_6, uniques_6 = get_feature_from_df(df_6, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "X_train_7, y_train_7, uniques_7 = get_feature_from_df(df_7, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "\n",
    "\n",
    "#this dataset has level 4 same as the other dataset levl 3\n",
    "\n",
    "X = pd.concat([ X_train_4, X_train_5,X_train_6,X_train_7], axis = 0)\n",
    "y = np.concatenate([y_train_4, y_train_5, y_train_6, y_train_7], axis = 0)\n",
    "\n",
    "X  = X[['20','21','40','41']]\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "onevsrest = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train, y_train).score(X_test, y_test)\n",
    "onevsone =  OneVsOneClassifier(LinearSVC(random_state=0)).fit(X_train, y_train).score(X_test, y_test)\n",
    "\n",
    "\n",
    "print(onevsrest)\n",
    "print(onevsone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no training_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_0, y_train_0, uniques_0 = get_feature_from_df(df_0, ['fzb'],['l1','l2','l3'], 0)\n",
    "X_train_1, y_train_1, uniques_1 = get_feature_from_df(df_1, ['fzb'],['l1','l2','l3'], 0)\n",
    "#X_train_2, y_train_2, uniques_2 = get_feature_from_df(df_2, ['fzb'],['l1','l2','l3'], 0)\n",
    "X_train_3, y_train_3, uniques_3 = get_feature_from_df(df_3, ['fzb'],['l1','l2','l4'], 0)\n",
    "X_train_4, y_train_4, uniques_4 = get_feature_from_df(df_4, ['fzb'],['l1','l2','l4'], 0)\n",
    "#X_train_5, y_train_5, uniques_5 = get_feature_from_df(df_5, ['fzb'],['l1','l2','l4'], 0)\n",
    "X_train_6, y_train_6, uniques_6 = get_feature_from_df(df_6, ['fzb'],['l1','l2','l4'], 0)\n",
    "X_train_7, y_train_7, uniques_7 = get_feature_from_df(df_7, ['fzb'],['l1','l2','l4'], 0)\n",
    "\n",
    "#this dataset has level 4 same as the other dataset levl 3\n",
    "y_train_3[y_train_3 ==5 ] = 4\n",
    "y_train_4[y_train_4 ==5 ] = 4\n",
    "#y_train_5[y_train_5 ==5 ] = 4\n",
    "y_train_6[y_train_6 ==5 ] = 4\n",
    "y_train_7[y_train_7 ==5 ] = 4\n",
    "\n",
    "X = pd.concat([X_train_0, X_train_1, X_train_3, X_train_4, X_train_6, X_train_7], axis = 0)\n",
    "y = np.concatenate([y_train_0, y_train_1, y_train_3,  y_train_4, y_train_6, y_train_7], axis = 0)\n",
    "\n",
    "X = X[['20','21','40','41']]\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "clf = LogisticRegressionCV(cv=5, random_state=0,max_iter = 10000, multi_class = 'auto').fit(X, y)\n",
    "\n",
    "print(clf.score(X, y))\n",
    "#print('coefficient: '+str(clf.coef_))\n",
    "#np.savetxt(sub+'day0_coef.csv', clf.coef_)\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "X_transform = lda.fit_transform(X,y)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "colors =  ['red', 'green', 'blue','yellow','purple']\n",
    "target_names = uniques_6[np.unique(y)]\n",
    "\n",
    "for color, i, target_name in zip(colors, np.unique(y), target_names):\n",
    "    plt.scatter(X_transform[y == i, 0], X_transform[y == i, 1], color=color, alpha=.8,# lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('lda')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "colors = ['red', 'green', 'blue','yellow','purple']\n",
    "target_names = uniques_6[np.unique(y)]\n",
    "\n",
    "for color, i, target_name in zip(colors, np.unique(y), target_names):\n",
    "    plt.hist(X_transform[y == i, 0], color=color, alpha=.8,# lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('lda')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_4, y_train_4, uniques_4 = get_feature_from_df(df_4, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "#X_train_5, y_train_5, uniques_5 = get_feature_from_df(df_5, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "X_train_6, y_train_6, uniques_6 = get_feature_from_df(df_6, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "X_train_7, y_train_7, uniques_7 = get_feature_from_df(df_7, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "\n",
    "\n",
    "#this dataset has level 4 same as the other dataset levl 3\n",
    "y_train_0[y_train_0 ==4 ] = 5\n",
    "y_train_1[y_train_1 ==4 ] = 5\n",
    "\n",
    "\n",
    "\n",
    "X = pd.concat([X_train_0, X_train_1, X_train_3, X_train_4, X_train_6,X_train_7], axis = 0)\n",
    "y = np.concatenate([y_train_0, y_train_1, y_train_3,  y_train_4, y_train_6, y_train_7], axis = 0)\n",
    "\n",
    "X  = X[['20','21','40','41']]\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "clf = LogisticRegressionCV(cv=5, random_state=0,max_iter = 10000, multi_class = 'auto').fit(X, y)\n",
    "\n",
    "print(clf.score(X, y))\n",
    "#print('coefficient: '+str(clf.coef_))\n",
    "#np.savetxt(sub+'day0_coef.csv', clf.coef_)\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "X_transform = lda.fit_transform(X,y)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "colors = ['red', 'green', 'blue','yellow','purple']\n",
    "target_names = uniques_6[np.unique(y)]\n",
    "\n",
    "for color, i, target_name in zip(colors, np.unique(y), target_names):\n",
    "    plt.scatter(X_transform[y == i, 0], X_transform[y == i, 1], color=color, alpha=.8,# lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('lda')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "colors = ['red', 'green', 'blue','yellow','purple']\n",
    "target_names = uniques_5[np.unique(y)]\n",
    "\n",
    "for color, i, target_name in zip(colors, np.unique(y), target_names):\n",
    "    plt.hist(X_transform[y == i, 0], color=color, alpha=.8,# lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('lda')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_4, y_train_4, uniques_4 = get_feature_from_df(df_4, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "X_train_5, y_train_5, uniques_5 = get_feature_from_df(df_5, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "X_train_6, y_train_6, uniques_6 = get_feature_from_df(df_6, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "X_train_7, y_train_7, uniques_7 = get_feature_from_df(df_7, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "\n",
    "\n",
    "X_train = X_train_4[['20','21','40','41']]\n",
    "y_train = y_train_4\n",
    "\n",
    "\n",
    "\n",
    "X_test  =  X_train_6[['20','21','40','41']]\n",
    "y_test = y_train_6\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train,y_train)\n",
    "\n",
    "print(lda.score(X_test, y_test))\n",
    "\n",
    "X_train_transform = lda.transform(X_train)\n",
    "X_test_transform = lda.transform(X_test)\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "colors = ['red', 'green', 'blue','yellow','purple']\n",
    "target_names = uniques_4[np.unique(y_train)]\n",
    "\n",
    "for color, i, target_name in zip(colors, np.unique(y_train), target_names):\n",
    "    plt.hist(X_train_transform[y_train == i, 0], color=color, alpha=.8,# lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('lda_train')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "colors = ['red', 'green', 'blue','yellow','purple']\n",
    "target_names = uniques_4[np.unique(y)]\n",
    "\n",
    "for color, i, target_name in zip(colors, np.unique(y_test), target_names):\n",
    "    plt.hist(X_test_transform[y_test == i, 0], color=color, alpha=.8,# lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('lda_test')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "colors = ['red', 'green', 'blue','yellow']\n",
    "target_names = ['20','21','40','41']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for color, target_name in zip(colors, target_names):\n",
    "    plt.hist(X[y==2][target_name], bins = 30, label=target_name ,  alpha=.8,)\n",
    "\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('level 1')\n",
    "plt.xlim((0, 0.8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for color, target_name in zip(colors, target_names):\n",
    "    plt.hist(X[y==3][target_name], bins = 30, label=target_name ,  alpha=.8,)\n",
    "\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('level 2')\n",
    "plt.xlim((0, 0.8))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for color, target_name in zip(colors, target_names):\n",
    "    plt.hist(X[y==4][target_name], bins = 30, label=target_name ,  alpha=.8,)\n",
    "\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('level 3')\n",
    "plt.xlim((0, 0.8))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for color, target_name in zip(colors, target_names):\n",
    "    plt.hist(X[y==5][target_name], bins = 30, label=target_name ,  alpha=.8,)\n",
    "\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('level 4')\n",
    "plt.xlim((0, 0.8))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for color, target_name in zip(colors, target_names):\n",
    "    plt.hist(X[y==6][target_name], bins = 30, label=target_name ,  alpha=.8,)\n",
    "\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('level 5')\n",
    "plt.xlim((0, 0.8))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train_4, y_train_4, uniques_4 = get_feature_from_df(df_4, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "#X_train_5, y_train_5, uniques_5 = get_feature_from_df(df_5, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "X_train_6, y_train_6, uniques_6 = get_feature_from_df(df_6, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "X_train_7, y_train_7, uniques_7 = get_feature_from_df(df_7, ['fzb'],['l1','l2','l3','l4','l5'], 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = pd.concat([X_train_4, X_train_6,X_train_7], axis = 0)\n",
    "y = np.concatenate([y_train_4, y_train_6, y_train_7], axis = 0)\n",
    "\n",
    "X = X[['20', '21', '40','41']]\n",
    "\n",
    "\n",
    "def oraculo(X, y, model, params={}):\n",
    "      (X_train, X_test, y_train, y_test) = train_test_split(X, y,          \n",
    "                                           test_size=.2, stratify=y,        \n",
    "                                           random_state= 3001)\n",
    "      pipeline = Pipeline([('column', StandardScaler()),\n",
    "                          ('model', model)])\n",
    "      print('Estimador: ', model)\n",
    "      grid = GridSearchCV(pipeline, params, \n",
    "                          scoring='neg_mean_absolute_error', \n",
    "                          n_jobs=-1, cv=3)\n",
    "      grid.fit(X_train, y_train)\n",
    "      pred = grid.best_estimator_.predict(X_test)\n",
    "      print('Mean Absolute Error: %1.4f' %    \n",
    "            (metrics.mean_absolute_error(y_test, pred)))\n",
    "      print('Accuracy: %1.4f\\n' % \n",
    "            (metrics.accuracy_score(y_test,   \n",
    "             np.round(pred).astype(int))))\n",
    "      print(metrics.classification_report(y_test,  \n",
    "             np.round(pred).astype(int)))\n",
    "      print('\\nDone!\\n\\n')\n",
    "\n",
    "\n",
    "\n",
    "models = [LinearRegression(), LogisticRegression(),\n",
    "          LogisticIT(), LogisticAT()]\n",
    "params = [{},{'model__max_iter': [100], 'model__C': [1.0]},   \n",
    "          {'model__max_iter': [100], 'model__alpha': [1.0]},\n",
    "          {'model__max_iter': [100], 'model__alpha': [1.0]}]\n",
    "\n",
    "for m,p in zip(models, params):\n",
    "         oraculo(X,\n",
    "                 y, m, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
