{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from mord import LogisticIT, LogisticAT\n",
    "\n",
    "from pyentrp import entropy as ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization\n",
    "def confusion_heat_map(model, X, y):\n",
    "    y_fit = model.predict(X)\n",
    "    import seaborn as sns\n",
    "    from matplotlib import pyplot as plt\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    mat = confusion_matrix(y, y_fit, normalize = 'true')\n",
    "    sns.heatmap(mat, square=True, annot=True, fmt='.2%', cbar='Blues',\n",
    "                xticklabels=np.unique(y),\n",
    "                yticklabels=np.unique(y))\n",
    "\n",
    "    plt.ylabel('true label')\n",
    "    plt.xlabel('predicted label');\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5 = pd.read_csv('D:/BCI_Classification/bci_fft_5freq_data_train.csv')\n",
    "\n",
    "X = df_5[['20','21','22','23','40','41','42','43']]\n",
    "y = df_5['y']\n",
    "day = df_5['day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(day)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify = day)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "clf = LogisticRegressionCV(cv=5, random_state=0,max_iter = 10000, multi_class = 'auto').fit(X_train, y_train)\n",
    "\n",
    "print(clf.score(X_test, y_test))\n",
    "print('coefficient: '+str(clf.coef_))\n",
    "\n",
    "confusion_heat_map(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## svm, grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify = day)\n",
    "\n",
    "\n",
    "#classification\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#pca = PCA(n_components=2, whiten=True)\n",
    "svc = SVC(kernel='rbf')\n",
    "model = make_pipeline(svc)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'svc__C': [1, 5, 10, 50],\n",
    "              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
    "grid = GridSearchCV(model, param_grid,cv = 5)\n",
    "\n",
    "%time grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "\n",
    "\n",
    "#scoring\n",
    "model = grid.best_estimator_\n",
    "y_fit = model.predict(X_test)\n",
    "\n",
    "\n",
    "confusion_heat_map(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify = day)\n",
    "\n",
    "\n",
    "#scoring: svm best:{'svc__C': 10, 'svc__gamma': 0.005}\n",
    "#pca = PCA(n_components=2, whiten=True)\n",
    "svc = SVC(kernel='rbf', C = 10, gamma = 0.005)\n",
    "model = make_pipeline(pca, svc)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_fit = model.predict(X_test)\n",
    "print(model.score(X_test, y_test))\n",
    "\n",
    "#visualization\n",
    "confusion_heat_map(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#pca = PCA(n_components=2, whiten=True)\n",
    "tree = DecisionTreeClassifier()\n",
    "model = make_pipeline(tree)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print('test score:{}'.format(model.score(X_test, y_test)))\n",
    "\n",
    "confusion_heat_map(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grid search decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "tree = DecisionTreeClassifier()\n",
    "model = make_pipeline(pca, tree)\n",
    "\n",
    "parameter_grid = {'decisiontreeclassifier__criterion': ['gini', 'entropy'],\n",
    "                  'decisiontreeclassifier__splitter': ['best', 'random'],\n",
    "                  'decisiontreeclassifier__max_depth': [1, 2, 3, 4, 5],\n",
    "                  'decisiontreeclassifier__max_features': [1, 2]}\n",
    "\n",
    "cross_validation = StratifiedKFold(n_splits=10)\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid=parameter_grid, cv=cross_validation)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "\n",
    "grid_search = grid_search.best_estimator_\n",
    "print('score for test set: {}'.format(grid_search.score(X_test, y_test)))\n",
    "\n",
    "cv_scores = cross_val_score(grid_search, X, y, cv=20)\n",
    "sns.distplot(cv_scores)\n",
    "plt.title('Average score: {}'.format(np.mean(cv_scores)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "## random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "tree = RandomForestClassifier(n_estimators=100)\n",
    "model = make_pipeline(pca, tree)\n",
    "\n",
    "parameter_grid = {'randomforestclassifier__criterion': ['gini', 'entropy'],\n",
    "                  'randomforestclassifier__max_depth': [1, 2, 3, 4, 5],\n",
    "                  'randomforestclassifier__max_features': [1, 2]}\n",
    "\n",
    "cross_validation = StratifiedKFold(n_splits=10)\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid=parameter_grid, cv=cross_validation)\n",
    "\n",
    "%time grid_search.fit(X_train, y_train)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "\n",
    "grid_search = grid_search.best_estimator_\n",
    "print('score for test set: {}'.format(grid_search.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "## random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "tree = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=5, max_features=1)\n",
    "model = make_pipeline(pca, tree)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print('score for test set: {}'.format(model.score(X_test, y_test)))\n",
    "\n",
    "confusion_heat_map(grid_search, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random forest without pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "## random forest random searchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "criterion = ['gini','entorpy']\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, \n",
    "                               verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "%time rf_random.fit(X_train, y_train)\n",
    "  \n",
    "\n",
    "print('Best score: {}'.format(rf_random.best_score_))\n",
    "print('Best parameters: {}'.format(rf_random.best_params_))\n",
    "\n",
    "rf_random = rf_random.best_estimator_\n",
    "print('score for test set: {}'.format(rf_random.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "## random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# 3 levels\n",
    "# tree = RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
    "#                        max_depth=40, max_features='auto', max_leaf_nodes=None,\n",
    "#                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#                        min_samples_leaf=1, min_samples_split=2,\n",
    "#                        min_weight_fraction_leaf=0.0, n_estimators=400,\n",
    "#                        n_jobs=None, oob_score=False, random_state=None,\n",
    "#                        verbose=0, warm_start=False)\n",
    "\n",
    "# 5 levels\n",
    "tree = RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=800,\n",
    "                       n_jobs=None, oob_score=False, random_state=None,\n",
    "                       verbose=0, warm_start=False)\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "print('score for test set: {}'.format(tree.score(X_test, y_test)))\n",
    "\n",
    "confusion_heat_map(tree, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.8, random_state=4)\n",
    "\n",
    "\n",
    "confusion_heat_map(tree, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross day validation (0% calibration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,5):#select day\n",
    "    chose_day = i\n",
    "\n",
    "    X_train = X[day != chose_day]\n",
    "    X_test = X[day ==  chose_day]\n",
    "\n",
    "    y_train = y[day!=chose_day]\n",
    "    y_test = y[day == chose_day]\n",
    "\n",
    "#random forest\n",
    "    tree.fit(X_train, y_train)\n",
    "    \n",
    "    print('day:{}'.format(i))\n",
    "    print('score for test set: {}'.format(tree.score(X_test, y_test)))\n",
    "    confusion_heat_map(tree, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross day validation(20% of calibration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(1,6):#select day\n",
    "    chose_day = i\n",
    "\n",
    "    X_train = X[day != chose_day]\n",
    "    X_test = X[day ==  chose_day]\n",
    "\n",
    "    y_train = y[day!=chose_day]\n",
    "    y_test = y[day == chose_day]\n",
    "\n",
    "    #include part of the test\n",
    "    X_pre_train, X_test, y_pre_train, y_test = train_test_split(X_test, y_test, test_size=0.9, random_state=4)\n",
    "\n",
    "    X_train = pd.concat([X_train, X_pre_train], axis = 0)\n",
    "    y_train = np.concatenate([y_train, y_pre_train])\n",
    "\n",
    "\n",
    "    ## random forest\n",
    "    tree.fit(X_pre_train, y_pre_train)\n",
    "    \n",
    "    print('day:{}'.format(i))\n",
    "    print('score for test set: {}'.format(tree.score(X_test, y_test)))\n",
    "    confusion_heat_map(tree, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross day validation (some levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,6):#select day\n",
    "    chose_day = i\n",
    "\n",
    "    X_train = X[day != chose_day]\n",
    "    X_test = X[day == chose_day]\n",
    "\n",
    "    y_train = y[day!=chose_day]\n",
    "    y_test = y[day == chose_day]\n",
    "\n",
    "    #include part of the test\n",
    "    X_pre_train = X_test[(y_test==2) | (y_test==4)| (y_test==5)| (y_test==6)]\n",
    "    y_pre_train = y_test[(y_test==2) | (y_test==4)| (y_test==5)| (y_test==6)]\n",
    "        \n",
    "    X_test = X_test[(y_test!=2) & (y_test!=4) & (y_test!=5) & (y_test!=6)]    \n",
    "    y_test = y_test[(y_test!=2) & (y_test!=4) & (y_test!=5) & (y_test!=6)]  \n",
    "    \n",
    "    X_train = pd.concat([X_train, X_pre_train], axis = 0)\n",
    "    y_train = np.concatenate([y_train, y_pre_train])\n",
    "\n",
    "\n",
    "    ## random forest\n",
    "    tree.fit(X_train, y_train)\n",
    "    \n",
    "    print('day:{}'.format(i))\n",
    "    print('score for test set: {}'.format(tree.score(X_test, y_test)))\n",
    "    confusion_heat_map(tree, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "# from tensorflow.keras.constraints import max_norm\n",
    "# import os\n",
    "# import glob\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    model = models.Sequential() \n",
    "    model.add(layers.Flatten(input_shape=(1, 4)))\n",
    "    \n",
    "    model.add(layers.Dense(64,activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "#     model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Dense(256,activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "#     model.add(layers.Dropout(0.2))\n",
    "    \n",
    "    model.add(layers.Dense(256,activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    \n",
    "    model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "    #model compilation\n",
    "    #Compile and train the model\n",
    "    model.compile(optimizer =keras.optimizers.Adam(lr = 0.01),\n",
    "                      loss = 'sparse_categorical_crossentropy',\n",
    "                      metrics=['acc'], verbose = 1)\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(neuron1=64, neuron2 = 256, neuron3 = 256, learn_rate=0.01, optimizer='adam'):\n",
    "    model = models.Sequential() \n",
    "    model.add(layers.Flatten(input_shape=(1, 4)))\n",
    "    \n",
    "    model.add(layers.Dense(neuron1,activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "#     model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Dense(neuron2,activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "#     model.add(layers.Dropout(0.2))\n",
    "    \n",
    "    model.add(layers.Dense(neuron3,activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    \n",
    "    model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "    #model compilation\n",
    "    #Compile and train the model\n",
    "    model.compile(optimizer = optimizer,\n",
    "                      loss = 'sparse_categorical_crossentropy',\n",
    "                      metrics=['acc'], verbose = 1)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model,epochs = 200, verbose=0)\n",
    "# define the grid search parameters\n",
    "batch_size = [30, 60, 100]\n",
    "optimizer = ['SGD', 'RMSprop', 'adam']\n",
    "learn_rate = [0.001, 0.01, 0.1]\n",
    "neuron1 = [64,128,256]\n",
    "neuron2 = [64,128,256]\n",
    "neuron3 = [64,128,256]\n",
    "\n",
    "\n",
    "\n",
    "param_grid = dict(batch_size=batch_size,learn_rate=learn_rate, optimizer=optimizer,\n",
    "                 neuron1 = neuron1, neuron2 = neuron2, neuron3 = neuron3)\n",
    "\n",
    "# rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, \n",
    "#                                verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "grid = RandomizedSearchCV(estimator=model, param_grid=param_grid, cv=5, n_iter = 100, random_state = 42, n_jobs = -1)\n",
    "\n",
    "X_train = np.expand_dims(X,axis=1)\n",
    "y_train = y.values - 2\n",
    "\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=2, stratify = y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=2,stratify = y_train )\n",
    "\n",
    "\n",
    "X_train = np.expand_dims(X_train,axis=1)\n",
    "X_test = np.expand_dims(X_test,axis=1)\n",
    "X_val = np.expand_dims(X_val,axis=1)\n",
    "\n",
    "y_train = y_train.values - 2\n",
    "y_test = y_test.values - 2\n",
    "y_val = y_val.values - 2 \n",
    "\n",
    "# checkpoint\n",
    "filepath=\"D://BCI_Classification//model_weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "#Compile and train the model\n",
    "model = create_model()\n",
    "history = model.fit(X_train, y_train,epochs=200, validation_data=(X_val, y_val), callbacks=callbacks_list)\n",
    "    \n",
    "#evaluation\n",
    "model.load_weights(filepath)\n",
    "test_loss,test_acc = model.evaluate(X_test,y_test)\n",
    "\n",
    "print('Model evaluation:{}'.format(test_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "day_value = day.values\n",
    "\n",
    "for train_day, predict_day in  KFold(5).split([0,1,2,3,4]):\n",
    "    \n",
    "    day_predict_idx = np.unique(day_value)[predict_day]\n",
    "    print(day_predict_idx)\n",
    "    train_list = (day_value != day_predict_idx)\n",
    "    \n",
    "    #training set\n",
    "    X_train = X.iloc[train_list==True , :]\n",
    "    y_train = y[train_list==True]\n",
    "       \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=2, stratify = y)\n",
    "    \n",
    "    #test set\n",
    "    X_test= X.iloc[train_list == False, :]\n",
    "    y_test= y[train_list==False]\n",
    "\n",
    "    X_train = np.expand_dims(X_train,axis=1)\n",
    "    X_test = np.expand_dims(X_test,axis=1)\n",
    "    X_val = np.expand_dims(X_val,axis=1)\n",
    "\n",
    "    y_train = y_train.values - 2\n",
    "    y_test = y_test.values - 2\n",
    "    y_val = y_val.values - 2 \n",
    "\n",
    "    # checkpoint\n",
    "    filepath=\"D://BCI_Classification//day_\"+str(day_predict_idx[0])+\"_model_weights.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    #Compile and train the model\n",
    "    model = create_model()\n",
    "    history = model.fit(X_train, y_train,epochs=100, validation_data=(X_val, y_val), callbacks=callbacks_list)\n",
    "\n",
    "    #evaluation\n",
    "    model.load_weights(filepath)\n",
    "    test_loss,test_acc = model.evaluate(X_test,y_test)\n",
    "\n",
    "    print('Model evaluation:{}'.format(test_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['acc'], label='accuracy')\n",
    "plt.plot(history.history['val_acc'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'], label = 'loss')\n",
    "plt.plot(history.history['val_loss'], label ='val_loss' )\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,6):\n",
    "    filepath=\"D://BCI_Classification//day_\"+str(i)+\"_model_weights.hdf5\"\n",
    "    #evaluation\n",
    "    model.load_weights(filepath)\n",
    "    \n",
    "    X_train = X[day!=i]\n",
    "    y_train = y[day!=i]\n",
    "    X_train = np.expand_dims(X_train,axis=1)\n",
    "    y_train = y_train.values - 2\n",
    "    \n",
    "    \n",
    "\n",
    "    X_test = X[day==i]\n",
    "    y_test = y[day==i]\n",
    "    X_test = np.expand_dims(X_test,axis=1)\n",
    "    y_test = y_test.values - 2\n",
    "    \n",
    "    \n",
    "    train_loss,train_acc = model.evaluate(X_train,y_train, verbose = 0)\n",
    "    test_loss,test_acc = model.evaluate(X_test,y_test, verbose = 0)\n",
    "    print('Predict day:{}'.format(i))\n",
    "    print('training set:{}'.format(train_acc))\n",
    "    print('test set:{}'.format(test_acc))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
